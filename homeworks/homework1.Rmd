---
title: "Homework 1"
author: "Ginevra Carbone"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(pandocfilters)
library(stats)
library(AER)
set.seed(123)
```

## Exercise 1

**TH-1:** Linear regression slides, slide 20. 

*Fit the same model with the function `bayesglm()` of the `arm` package and comment. Then, use an informative prior and comment the results.*

```{r}
# non informative prior
glm_noninf <- bayesglm(dist~speed, data=cars, family=gaussian,
                       prior.scale=Inf, prior.df=1)
display(glm_noninf)

# informative prior
glm_inf <- bayesglm(dist ~ speed, data=cars, family=gaussian,
                    prior.mean=0.01, prior.scale=0.5, 
                    prior.mean.for.intercept = 0.01, 
                    prior.scale.for.intercept = 0.5)
display(glm_inf)

plot(dist ~ speed, data=cars)
abline(a=glm_noninf$coefficients[1], 
       b=glm_noninf$coefficients[2], col="red")
abline(a=glm_inf$coefficients[1], 
       b=glm_inf$coefficients[2], col="blue")

```

We can see from both the summaries and the overlapping lines in the plot that informative and non informative priors lead to the same result. This happens because the data itself is very informative and the likelihood dominates over the priors.

## Exercise 2

**BDA-1:** Ex 2.7 (p. 58) - Noninformative prior densities.

*(a) For the binomial likelihood, $y ∼ Bin(n, θ)$, show that $p(θ) ∝ θ ^{−1} (1 − θ)^{−1}$ is the uniform prior distribution for the natural parameter of the exponential family.*

(a) The binomial likelihood can be written in exponential form as 
$$
p(y|\theta)= {n \choose y} \theta^y(1-\theta)^{n-y} \propto  {n \choose y}exp\Big\{y \cdot log\frac{\theta}{1-\theta}+n\cdot log(1-\theta)\Big\}
$$
with natural parameter $\eta:=\phi(\theta)=log\frac{\theta}{1-\theta}$.
Assuming a uniform prior distribution on $\eta$, $\theta$ is distributed according to
$$
p(\theta) 
= p(\eta(\theta))\Bigg|\frac{d\eta(\theta)}{d\theta}\Bigg|
= I_\eta \cdot \frac{1}{\theta(1-\theta)} 
= \theta^{-1} (1-\theta)^{-1}
$$

The posterior on $\theta$ becomes
$$
\pi(\theta|y) \propto \theta^{-1} (1-\theta)^{-1} \theta^y (1-\theta)^{n-y}
\propto \theta^{y-1} (1-\theta)^{n-y-1}\\
\theta | y \sim Beta(y,n-y)
$$

*(b) Show that if $y = 0$ or $n$, the resulting posterior distribution is improper.*

$$
\int_0 ^1 \pi(\theta | y=0) d\theta 
= \int_0 ^1 \theta^{-1} d\theta
= lim_{c \rightarrow 0_+} [log \theta]_c^1 = + \infty\\
\int_0 ^1 \pi(\theta | y=1) d\theta 
= \int_0 ^1 (1-\theta)^{-1} d\theta 
= \int_0 ^1 \rho^{-1} d\rho = +\infty
$$


## Exercise 3

**BDA-2:** Ex 2.8 (p. 58) - Normal distribution with unknown mean.

*A random sample of $n$ students is drawn from a large population, and their weights are measured. The average weight of the $n$ sampled students is $y = 150$ pounds. Assume the weights in the population are normally distributed with unknown mean $θ$ and known standard deviation 20 pounds. Suppose your prior distribution for $θ$ is normal with mean 180 and standard deviation 40.*

*(a) Give your posterior distribution for $θ$. (Your answer will be a function of $n$.)*

Prior $\theta \sim \mathcal{N}(\theta_0, \sigma_0^2) = \mathcal{N}(180,40^2)$ gives $\pi(\theta) \propto exp\Big(-\frac{(\theta-180)^2}{2 \cdot 40^2}\Big)$.

Likelihood $y | \theta \sim \mathcal{N}(\theta, \sigma^2) = \mathcal{N}(\theta, 20^2)$ gives 

\begin{align}

p(y|\theta) 
  & = \Pi_{i=1}^n f_Y(y_i; \theta, 20^2) = \Pi_{i=1}^n \frac{1}{\sqrt{2 \pi 20^2}} exp\Big(-\frac{(y_i - \theta)^2}{2\cdot 20^2}\Big) \\
  & \propto exp \Big(\sum(y_i - \theta)^2 \Big)  \propto exp \Big( -2\theta \cdot 150 n + n \theta^2 \Big)
  
\end{align}

So we get the posterior 

$$
\pi(\theta | y) \propto p(y | \theta) \pi(\theta) \propto exp\Bigg(-\frac{(\theta-180)^2}{2\cdot 40^2}-2 \theta \cdot 150n+n \theta^2\Bigg)
$$

which, after proper calculations, becomes the following normal distribution

\begin{align}
\theta | y \sim
\mathcal{N}(\theta_0',\sigma_0')
&= \mathcal{N}
\Bigg( 
\frac{180 \cdot 20^2+150 \cdot n \cdot 20^2}{20^2 + n\cdot 40^2}, 
\frac{20^2 \cdot 40^2}{20^2+n40^2}
\Bigg)\\

&= \mathcal{N}\Bigg(\frac{72000+240000 n}{400+1600n}, \frac{640000}{400+1600n}\Bigg) 
\end{align}

*(b) A new student is sampled at random from the same population and has a weight of $\tilde{y}$ pounds. Give a posterior predictive distribution for $\tilde{y}$. (Your answer will still be a function of $n$.)*

The posterior predictive distribution on the observed data $y$ is

$$
p(\tilde{y} | y) = \int_{\Theta} p(\tilde{y} | \theta) \pi(\theta|y)d\theta 
$$

so in our case

$$
\tilde{y} | y
  \sim  \mathcal{N} (\tilde{y} | \theta_0',\sigma_0'+\sigma^2)
  = \mathcal{N}\Bigg(\frac{72000+240000 n}{400+1600n}, \frac{640000}{400+1600n}+400\Bigg)
$$

*(c) For $n = 10$, give a 95% posterior interval for $θ$ and a 95% posterior predictive interval for $\tilde{y}$.*

The posterior interval for $\theta$ is the interval whose extremes are the $\alpha/2=0.025$ and $1-\alpha/2=0.975$ quantiles of $\pi(\theta | y)$. The same holds for $y$ and the quantiles of $p(\tilde{y},y)$.

```{r}
theta_l = qnorm(.025, 150.731707317, sqrt(39.024390244))
theta_r = qnorm(.975, 150.731707317, sqrt(39.024390244))
cat("posterior interval for theta: [", theta_l, "," , theta_r, "]")

theta_l = qnorm(.025, 150.731707317, sqrt(439.024390244))
theta_r = qnorm(.975, 150.731707317, sqrt(439.024390244))
cat("posterior predictive interval for y_tilde: [", theta_l, "," , theta_r, "]")
```

*(d) Do the same for n = 100.*

```{r}
theta_l = qnorm(.025, 150.074812968, sqrt(3.990024938))
theta_r = qnorm(.975, 150.074812968, sqrt(3.990024938))
cat("posterior interval for theta: [", theta_l, "," , theta_r, "]")

theta_l = qnorm(.025, 150.074812968, sqrt(403.990024938))
theta_r = qnorm(.975, 150.074812968, sqrt(403.990024938))
cat("posterior predictive interval for y_tilde: [", theta_l, "," , theta_r, "]")

```

## Exercise 4

**LAB-1:** *Repeat the lab session using the dataset `ShipAccidents` in the AER package.*

**1.** *Plot the four prior distributions on the same graph, then add the Jeffreys’ prior.*

```{r}
data("ShipAccidents")
head(ShipAccidents)

theta <- 0.5
a <- c(1, 1, 1, 2)
b <- c(.5, 2, 10, 2)

gamma_prior <- function(x, k){return(dgamma(x, a[k], b[k]))}
jeffreys_prior <- function(x, theta){return(1/sqrt(x))}

plot(c(), c(), xlim=c(0,3), ylim=c(0,5), xlab="", ylab="", main="Prior distributions")
for(k in 1:4) curve(gamma_prior(x,k), add=T, col=k)
curve(jeffreys_prior(x, theta), add=T, col=6)
legend(2, 2, 
       legend=c("Gamma(1,0.5)", "Gamma(1,2)", 
                "Gamma(1,10)", "Gamma(2,2)",
                "Jeffreys"),
       col=c(1:4,6), lty=1, cex=0.8)
```

**2.** *For each dataset, plot on the same graph the five posterior distributions.*

```{r}
rm(gamma_posterior, jeffreys_posterior, plot_posteriors)

incidents <- ShipAccidents[,"incidents"]
service <- ShipAccidents[,"service"]

gamma_posterior <- function(x,y,t,k){
    return(dgamma(x, sum(y)+a[k], sum(t)+b[k]))
}

jeffreys_posterior <- function(x,y,t,theta){
    return(dgamma(x, sum(y)+theta, sum(t)))
}


plot_posteriors <- function(y, t){
    plot(NULL, xlim=c(0.001,0.0035), ylim=c(0,1), 
         main=paste("Posteriors, n samples =",length(y)), xlab="", ylab="")
  
    for(k in 1:5) curve(gamma_posterior(x,y,t,k)/mean(t), n=1000, add=T, col=k)
    curve(jeffreys_posterior(x,y,t,theta)/mean(t), n=1000, add=T, col=6)
    
    legend(0.0025, 0.8, legend=c("Gamma(1,0.5)", "Gamma(1,2)", 
                            "Gamma(1,10)", "Gamma(2,2)","Jeffreys"),
           col=c(1:4,6), lty=1, cex=0.8)
}

plot_posteriors(incidents[1:10], service[1:10])
plot_posteriors(incidents[1:20], service[1:20])
plot_posteriors(incidents, service)
```

**3.** *Find the posterior expectations and the maxima a posteriori (MAP) of $\theta$.*

```{r}
posterior_expectations <- function(y, t){
    for(k in 1:4) {
      cat(c("Gamma(",a[k], ",",b[k], "):",
            (sum(y)+a[k])/(sum(t)+b[k]),"\n"), sep=" ")
    }
    cat(c("Jeffreys(",theta,"): ", 
          (sum(y)+theta)/sum(t),"\n"), sep="")
}
posterior_expectations(incidents, service)

MAP <- function(y, t){
    for(k in 1:4) {
      cat(c("Gamma(",a[k], ",",b[k], "):",
            (sum(y)+a[k]-1)/(sum(t)+b[k]),"\n"), sep=" ")
    }
    cat(c("Jeffreys(",theta,"): ", 
          (sum(y)+theta-1)/sum(t),"\n"), sep="")
}
MAP(incidents, service)
```


**4.** *Comment on plots and estimations.*

The posteriors model the monthly rate of incidents.
We can notice that as the sample size increases, the posterior distributions become more data-dependent, hence the prior is less relevant.

**5.** *Construct the 95% credible intervals, equi-tails and highest posterior density (HPD), for the three datasets using the Jeffreys’ prior.*

```{r}
# fix confidence level
alpha <- 0.05

credible_intervals <- function(alpha,y,t) {
    return(qgamma(c(alpha*0.5, 1-alpha*0.5), sum(y)+theta, sum(t)))
}

credible_intervals(alpha,incidents,service)

hpd <- function(y,t,p){
    set.seed(123)
    y <- rgamma(1000,sum(y)+theta,sum(t))
    dy<-density(y)
    md<-dy$x[dy$y==max(dy$y)]
    py<-dy$y/sum(dy$y)
    pys<-sort(py, decreasing=T)
    ths<-min(pys[cumsum(pys)<p])
    list(hpdr=range(dy$x[py>=ths]),mode=md)
}

hpd(incidents,service,0.95)
```


