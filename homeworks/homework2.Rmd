---
title: "Homework 2"
author: "Ginevra Carbone"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1

**TH-1:** Markov Chain - Given the transition matrix P (slide 14 of MCMC slides) of a Markov chain, follow the steps described in slide 15 and verify that the stationary distribution is $\pi=(0.1,0.2,0.2,0.2,0.2,0.1)$ (slide 16).

We simply have to solve the system $\pi = \pi P$ for the given transition matrix. Suppose $\pi=(a,b,c,d,e,f)$, then
\begin{align}
a&= 0.5a+0.25b\\
b&= 0.5a+0.5b+0.25c\\
c& = 0.25b+0.5c+0.25d\\
d&=0.25c+0.5d+0.25e\\
e&=0.25d+0.5e+0.5f\\
f&=0.25e+0.5f
\end{align}

gives
\begin{align}
a &=f\\
b &=c=d=e=2a
\end{align}

By imposing $a+b+c+d+e+f=1$ we finally get $\pi=(0.1,0.2,0.2,0.2,0.2,0.1)$.

## Exercise 2

**TH-2:** An insect deposits $N$ eggs and each one is seeded with probability $\theta$. The probability $\theta$ varies from insect to insect, as well as the number of deposited eggs. Assuming that the number of deposited eggs $N$ follows a Poisson distribution of parameter $\lambda$ and that the probability of insemination $\theta$ follows a Beta distribution of parameters $(\alpha,\beta)$, with $N$ and $\theta$ independent of each other:

**2a)** determine the distributions of each of the 3 conditioned variables to the other 2. In particular, show that:

- $X|N=n,\theta=t\sim Binomial(n,t)$;
- $\theta|N=n,X=x\sim Beta(\alpha+x,\beta+n−x)$;
- $N−x|X=x,\theta=t\sim Poisson(\lambda(1−\theta))$.
  
The distribution of $X$ can be modeled as a $Bin(n,t)$ simply because it counts the number of seeded eggs over $n$ deposited ones, each one with probability $t$ of being seeded.

Regarding the other conditioned variables, the following proportionalities hold:

\begin{align}
p(\theta|X,N) &\propto p(X|\theta,N) p(\theta,N)\\
              &=   p(X|\theta, N) p(\theta)p(N) \\
              &= {n \choose x} t^x (1-t)^{n-x} \frac{\lambda^n}{n!} e^{-\lambda} \frac{t^{\alpha-1}(1-t)^{\beta-1}}{B(\alpha,\beta)}\\
              &\propto t^{\alpha+x-1} (1-t)^{\beta+n-x-1}\\
\end{align}
so $\theta|N=n,X=x \sim Beta(\alpha+x,\beta+n-x)$.

\begin{align}
p(N|X,\theta) &\propto {n \choose x} (1-t)^{n-x} \frac{\lambda^n}{n!}\\
              &= \frac{1}{(n-x)!} (1-t)^{n-x} \lambda^n\\
              &\propto \frac{[\lambda(1-t)^{n-x}]}{(n-x)!} e^{-\lambda(1-t)}
\end{align}
so  $N−x|X=x,\theta =t \sim Poisson(\lambda(1−\theta))$.


**2b)** Draw, using Gibbs sampler, a sample of $(n,\theta,x)$ and graphically represent the distribution of seeded eggs $X$, then compare the theoretical mean and variance of $X$ with the ones computed on the simulated sample.

```{r}

init = list(lambda=10, alpha=2, beta=8, n=1.0, theta=0.5, x=1.0)

gibbs_sampler = function(nsims, init){
  
  # params = (n, theta, x)
  n = rep(init$n,nsims)
  theta = rep(init$theta,nsims)
  x = rep(init$x,nsims)
  
  for (i in 2:nsims){
    n[i] = rpois(1, init$lambda*(1-theta[i-1]))
    theta[i] = rbeta(1, init$alpha+x[i-1], init$beta+n[i]-x[i-1])
    x[i] = rbinom(1, n[i], theta[i])
  }

  plot(n, theta, type="l", xlab=expression(N), ylab=expression(Theta))
  plot(theta, x, type="l", xlab=expression(Theta), ylab=expression(X))
  
  simulations = list(n=n, theta=theta, x=x)
  return(simulations)
}

simulations = gibbs_sampler(nsims=10000, init)

relative_frequency = table(simulations$x)/length(simulations$x)

#barplot(relative_frequency)
plot(relative_frequency, main=paste("distribution of X"))
```

The theoretical mean and variance for $X$ are the first two moments of the compound probability distribution $p_X(X)=p(X|\theta,N)p(\theta,N)$:

\begin{align}
E[X] &= E[E[X|\theta,N]] \\
     &= E[\theta N]\\
Var(X) &= E[Var(X|\theta,N)]+Var(E[X|\theta,N])\\
       &= E[N\theta(1-\theta)]+Var(N\theta)
\end{align}


```{r}
# simulated mean for X
mean(simulations$x)

# theoretical mean for X
mean(simulations$theta*simulations$n)

# simulated variance for X
var(simulations$x)

# theoretical variance for X
mean(simulations$n*simulations$theta*(1-simulations$theta))+
  var(simulations$n*simulations$theta)
```


**2c)** Verify that the simulated conditional distribution of $\theta$ given $N=\hat{n}$ and $X=\hat{x}$, where $\hat{n}$ and $\hat{x}$ are posterior estimates, corresponds with the theoretical one.

```{r}
hist(simulations$theta, breaks = 100, prob=T, col="grey", xlim=c(0,1), xlab="", ylab="", main="distribution of theta")
curve(dbeta(x, init$alpha, init$beta), add=T, col="red")

```



## Exercise 3

**MCR-1:** Ex 6.14 (p.196) - Random walk Metropolis-Hastings.

**3a)** Reproduce the graphs in Figure 6.7 for different values of $\delta$. Explore both small and large $\delta$’s. Can you find an optimal choice in terms of autocovariance?

```{r}
set.seed(123)

metrop_hastings = function(cand_distr, param, n_sims, burn_in){
  draws = c()
  # set first draw
  draws[1] = 0
  accepted = 0
  for(i in 2:n_sims){
    current = draws[i-1]
    candidate = current + 
      cand_distr(1, param[1], param[2])
    accept_ratio = exp((current^2-candidate^2)/2)
    accept_prob = min(accept_ratio, 1)
    #accept_prob = dnorm(candidate,0,1)/dnorm(current,0,1)
    
    if (runif(1) <= accept_prob){
      draws[i] = candidate
      accepted = accepted+1
    }
    else
      draws[i] = current
  }
  
  print(paste(
    #deparse(substitute(cand_distr)),
    "(",param[1],",",param[2],")",
    "acceptance rate: ", accepted/n_sims))

  return(draws[burn_in:n_sims])
}

# simulate samples
simulate = function(cand_distr, params, n_sims, burn_in=1){
  simulations = c()
  len = length(params)
  for(i in 1:len)
    simulations[[i]] = metrop_hastings(cand_distr, params[[i]], n_sims, burn_in)
  
  # plots
  par(mfrow=c(1,len))#, mai = rep(.3, len+1))
  
  # chains
  for(i in 1:len)
    plot(simulations[[i]], 
         xlim=c(2000, 5000), ylim=c(-4,4), type='l', 
         xlab = "", ylab = "", 
         main= paste(
           deparse(substitute(cand_distr)),
           "(",params[[i]][1],",",params[[i]][2],")"))
  
  # histograms
  for(i in 1:len){
    hist(simulations[[i]], breaks = 50, prob=T, 
         col="grey", xlim=c(-4,4), 
         xlab="", ylab="", main="")
    curve(dnorm(x), add=T, col="red")
  }
  
  # ACF
  for(i in 1:len)
    acf(simulations[[i]], main="")
}

params =  list(c(-.1,.1), c(-1,1), c(-10,10))
simulate(runif, params=params, n_sims=5000)

```

The autocorrelation should become smaller as the number of steps increases, since otherwise the algorithm is not able to explore the whole parameter space. So the optimal choice in terms of the ACF plots from the book is the third one.

```{r}
params = list(c(-4,4), c(-5,5), c(-6,6))
simulate(runif, params=params, n_sims=10000, burn_in=1000)
```

These last plots show $10000$ simulations with burn in on the first $1000$ ones, for more appropriate values for $\delta$. We can notice a nice convergence, very small peaks in the histograms and a decreasing autocorrelation.

**3b)** The random walk candidate can be based on other distributions. Consider generating a $\mathcal{N}(0,1)$ distribution using a random walk with a (i) Cauchy candidate, and a (ii) Laplace candidate. Construct these Metropolis–Hastings algorithms and compare them with each other and with the Metropolis–Hastings random walk with a uniform candidate.

```{r message=FALSE}
library(rmutil)
par(mfrow=c(3,3))
simulate(runif, params=list(c(-6,6)), n_sims=10000, burn_in=1000)
simulate(rcauchy, params=list(c(0,3)), n_sims=10000, burn_in=1000)
simulate(rlaplace, params=list(c(0,5)), n_sims=10000, burn_in=1000)
```

For a small scale, the acceptance rate is very close to one since the generated points are close to each other. 
By running a few simulations we can easily spot the right coeffiecients. The behaviour is very similar for all the candidates: they are all stable, converging to the same mean, with small peaks and vanishing autocovariance.

**3c)** For each of these three random walk candidates, examine whether or not the acceptance rate can be brought close to $0.25$ for the proper choice of parameters.

```{r message=FALSE}
library(rmutil)
# uniform
params = list(c(-6.6,6.6))
simulate(runif, params=params, n_sims=5000)
# cauchy
params = list(c(0,3.5))
simulate(rcauchy, params=params, n_sims=5000)
# laplace
params = list(c(0,5.1))
simulate(rlaplace, params=params, n_sims=5000)
```


